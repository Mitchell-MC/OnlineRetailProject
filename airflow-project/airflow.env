# Core Airflow Configuration
AIRFLOW__CORE__EXECUTOR=CeleryExecutor
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0
AIRFLOW__CORE__FERNET_KEY=OnfNZiO4pfvIuVIt4EMMGx_bJasFN53hlZMBKYi-PgU=
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__API__AUTH_BACKEND='airflow.api.auth.backend.basic_auth'
AIRFLOW__CORE__ALLOWED_DESERIALIZATION_CLASSES=airflow.* astro.* 

# Java Configuration (required for Spark)
JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

# Astro SDK Connection for Snowflake (use environment variables)
AIRFLOW_CONN_SNOWFLAKE_DEFAULT='{
    "conn_type": "snowflake",
    "login": "${SNOWFLAKE_USER}",
    "password": "${SNOWFLAKE_PASSWORD}",
    "schema": "${SNOWFLAKE_SCHEMA}",
    "extra": {
        "account": "${SNOWFLAKE_ACCOUNT}",
        "database": "${SNOWFLAKE_DATABASE}",
        "warehouse": "${SNOWFLAKE_WAREHOUSE}"
    }
}'

# AWS Connection (for S3 access) - use environment variables
AIRFLOW_CONN_AWS_DEFAULT='{
    "conn_type": "aws",
    "login": "${AWS_ACCESS_KEY_ID}",
    "password": "${AWS_SECRET_ACCESS_KEY}",
    "extra": {
        "region_name": "${AWS_REGION}"
    }
}'

# Set the user and group for files created in mounted volumes
AIRFLOW_UID=50000
AIRFLOW_GID=0
