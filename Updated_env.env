# ===================================================================
# Airflow Core Configuration
# ===================================================================
# Set the user and group for files created in mounted volumes
AIRFLOW_UID=50000
AIRFLOW_GID=0

# ===================================================================
# AWS Connection (for S3 access)
# Airflow will automatically create a connection named 'aws_default'
# ===================================================================
AIRFLOW_CONN_AWS_DEFAULT='{
    "conn_type": "aws",
    "login": "YOUR_AWS_ACCESS_KEY_ID",
    "password": "YOUR_AWS_SECRET_ACCESS_KEY",
    "extra": {
        "region_name": "us-east-1"
    }
}'

# ===================================================================
# Snowflake Connection
# Airflow will automatically create a connection named 'snowflake_default'
# ===================================================================
AIRFLOW_CONN_SNOWFLAKE_DEFAULT='{
    "conn_type": "snowflake",
    "login": "YOUR_SNOWFLAKE_USER",
    "password": "YOUR_SNOWFLAKE_PASSWORD",
    "extra": {
        "account": "YOUR_SNOWFLAKE_ACCOUNT_LOCATOR",
        "warehouse": "COMPUTE_WH",
        "database": "ECOMMERCE_DB",
        "role": "ACCOUNTADMIN",
        "region": "us-east-1",
        "schema": "ANALYTICS"
    }
}'

# ===================================================================
# Spark Connection (for SparkSubmitOperator)
# Airflow will automatically create a connection named 'spark_default'
# ===================================================================
AIRFLOW_CONN_SPARK_DEFAULT='{
    "conn_type": "spark",
    "host": "spark://spark-master",
    "port": 7077,
    "extra": {
        "master": "spark://spark-master:7077"
    }
}'